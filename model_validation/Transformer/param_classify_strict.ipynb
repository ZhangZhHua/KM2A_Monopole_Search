{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c8859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum \n",
    "# 225.0722880973707,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62554af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f3a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Loading: /home/zhonghua/data/Dataset_Filted/filted_Monopole_1e10_merged_1000_70_new_dataset_strict_300_500.npz\n",
      "Loading: /home/zhonghua/data/Dataset_Filted/CosmicRay/Npz/Proton_1000_70_1e10_V03_dataset_strict_300_500.npz\n",
      "Loading: /home/zhonghua/data/Dataset_Filted/Experiment/2022/all_combined_2022_dataset_1e10_for_diffmodels_strict_300_500.npz\n",
      "Train size: 29845, Val size: 7462\n",
      "\n",
      "=== Training XGBoost ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhonghua/miniconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [20:57:42] WARNING: /croot/xgboost-split_1749630910898/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving CSV: /home/zhonghua/Filt_Event/model_validation/xgb_mlp_strict_300_500/validation_set_xgb.csv\n",
      "Saving CSV: /home/zhonghua/Filt_Event/model_validation/xgb_mlp_strict_300_500/Exp_2022_xgb.csv\n",
      "Plot saved to /home/zhonghua/Filt_Event/model_validation/xgb_mlp_strict_300_500/XGBoost_dist.png\n",
      "\n",
      "All Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据加载与预处理工具\n",
    "# ==========================================\n",
    "\n",
    "def load_full_data(npz_file):\n",
    "    \"\"\"\n",
    "    加载完整的 npz 数据，返回包含所有 params 的 DataFrame。\n",
    "    \"\"\"\n",
    "    print(f\"Loading: {npz_file}\")\n",
    "    data = np.load(npz_file, allow_pickle=True)\n",
    "    # 使用 param_names 作为列名构建完整 DataFrame\n",
    "    df = pd.DataFrame(data[\"params\"], columns=data[\"param_names\"])\n",
    "    return df\n",
    "\n",
    "def prepare_data(sig_file, bkg_file, test_files_dict, features, test_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    准备训练、验证和测试数据。\n",
    "    同时返回用于保存 CSV 的原始 DataFrame (仅 Val 和 Test，不含 Train)。\n",
    "    \"\"\"\n",
    "    # 1. 加载完整数据\n",
    "    df_sig = load_full_data(sig_file)\n",
    "    df_bkg = load_full_data(bkg_file)\n",
    "    \n",
    "    # 检查特征是否存在\n",
    "    for f in features:\n",
    "        if f not in df_sig.columns:\n",
    "            raise ValueError(f\"Feature {f} missing in signal file\")\n",
    "        if f not in df_bkg.columns:\n",
    "            raise ValueError(f\"Feature {f} missing in background file\")\n",
    "\n",
    "    # 创建 Label\n",
    "    y_sig = np.ones(len(df_sig))\n",
    "    y_bkg = np.zeros(len(df_bkg))\n",
    "    \n",
    "    # 合并 DataFrame 和 Label\n",
    "    df_all = pd.concat([df_sig, df_bkg], axis=0).reset_index(drop=True)\n",
    "    y_all = np.concatenate([y_sig, y_bkg], axis=0)\n",
    "    \n",
    "    # 提取训练所需的特征矩阵 X\n",
    "    X_all = df_all[features].values\n",
    "    \n",
    "    # 划分 Train / Val\n",
    "    # 注意：这里我们同时划分 X(特征), y(标签) 和 df_all(原始完整数据)\n",
    "    # random_state 保证了 X 和 df 的划分索引是一一对应的\n",
    "    X_train, X_val, y_train, y_val, _, df_val = train_test_split(\n",
    "        X_all, y_all, df_all, test_size=test_size, random_state=seed, stratify=y_all\n",
    "    )\n",
    "    \n",
    "    # 2. 加载外部测试数据\n",
    "    # test_data 结构: { 'name': (X_matrix, full_DataFrame) }\n",
    "    test_data = {}\n",
    "    for name, path in test_files_dict.items():\n",
    "        df_temp = load_full_data(path)\n",
    "        X_temp = df_temp[features].values\n",
    "        test_data[name] = (X_temp, df_temp)\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val, df_val, test_data\n",
    "\n",
    "def save_to_csv(df_original, scores, out_dir, filename):\n",
    "    \"\"\"\n",
    "    将原始数据与模型评分合并并保存为 CSV\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    save_path = os.path.join(out_dir, filename)\n",
    "    \n",
    "    # 避免修改原始内存中的 df，创建副本\n",
    "    df_save = df_original.copy()\n",
    "    \n",
    "    # 添加模型分数\n",
    "    df_save['model_score'] = scores\n",
    "    \n",
    "    # 保存\n",
    "    print(f\"Saving CSV: {save_path}\")\n",
    "    df_save.to_csv(save_path, index=False, float_format='%.6g')\n",
    "\n",
    "# ==========================================\n",
    "# 2. 绘图函数 (保持不变)\n",
    "# ==========================================\n",
    "def plot_score_distributions(val_dict, test_dict, out_dir, model_name):\n",
    "    \"\"\"\n",
    "    val_dict: {'sig': scores, 'bkg': scores}\n",
    "    test_dict: {'EF': scores, 'QF': scores, ...}\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    bins = np.linspace(0, 1, 21) # 稍微增加一点bin数使其更平滑\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    \n",
    "    hist_val_bkg, _ = np.histogram(val_dict['bkg'], bins=bins, density=True)\n",
    "    hist_base = hist_val_bkg.copy()\n",
    "    hist_base[hist_base == 0] = np.nan\n",
    "    \n",
    "    fig, (ax_main, ax_ratio) = plt.subplots(2, 1, figsize=(10, 8), sharex=True, \n",
    "                                            gridspec_kw={'height_ratios': [3, 1], 'hspace': 0.05})\n",
    "    \n",
    "    colors = ['tab:blue', 'tab:green', 'tab:orange', 'tab:purple', 'tab:red']\n",
    "    \n",
    "    # --- PDF ---\n",
    "    ax_main.hist(val_dict['sig'], bins=bins, density=True, histtype='step', \n",
    "                 color='red', linestyle='--', linewidth=1.5, label='Val Sig PDF')\n",
    "    \n",
    "    ax_main.hist(bins[:-1], bins, weights=hist_val_bkg, histtype='step', \n",
    "                 color='black', linestyle='--', linewidth=1.5, label='Val Bkg PDF')\n",
    "    \n",
    "    for i, (name, scores) in enumerate(test_dict.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        hist_curr, _ = np.histogram(scores, bins=bins, density=True)\n",
    "        ax_main.hist(bins[:-1], bins, weights=hist_curr, histtype='step', \n",
    "                     alpha=0.3, color=color, label=f\"{name} (PDF)\")\n",
    "        ax_main.hist(bins[:-1], bins, weights=hist_curr, histtype='step', \n",
    "                     linewidth=1, color=color)\n",
    "        \n",
    "        ratio = hist_curr / hist_base\n",
    "        ax_ratio.step(bin_centers, ratio, where='mid', color=color, linewidth=1.5)\n",
    "\n",
    "    # --- CDF ---\n",
    "    ax_cdf = ax_main.twinx()\n",
    "    sig_sorted = np.sort(val_dict['sig'])\n",
    "    y_sig = np.arange(1, len(sig_sorted) + 1) / len(sig_sorted)\n",
    "    ax_cdf.plot(sig_sorted, y_sig, color='red', linestyle='--', alpha=0.3, linewidth=2, label='Val Sig CDF')\n",
    "\n",
    "    bkg_sorted = np.sort(val_dict['bkg'])\n",
    "    y_bkg = 1.0 - np.arange(1, len(bkg_sorted) + 1) / len(bkg_sorted)\n",
    "    ax_cdf.plot(bkg_sorted, y_bkg, color='black', linestyle='--', alpha=0.3, linewidth=2, label='Val Bkg Surv')\n",
    "\n",
    "    ax_main.set_ylabel(\"Density\")\n",
    "    # ax_main.set_yscale('log') # Log scale 经常更好看\n",
    "    ax_main.set_title(f\"{model_name} Score Distributions\")\n",
    "    ax_main.legend(loc='upper center', ncol=2, frameon=False, fontsize=10)\n",
    "    ax_main.grid(alpha=0.3)\n",
    "    \n",
    "    ax_cdf.set_ylabel(\"Probability\")\n",
    "    ax_cdf.set_ylim(0, 1.05)\n",
    "    \n",
    "    ax_ratio.axhline(1, color='black', linestyle='--', alpha=0.5) \n",
    "    ax_ratio.set_ylabel(\"Ratio / Val Bkg\")\n",
    "    ax_ratio.set_xlabel(\"Model Score\")\n",
    "    ax_ratio.set_ylim(0, 3) \n",
    "    ax_ratio.grid(alpha=0.3)\n",
    "    \n",
    "    save_path = os.path.join(out_dir, f\"{model_name}_dist.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. 模型定义与运行函数\n",
    "# ==========================================\n",
    "\n",
    "# --- XGBoost ---\n",
    "def run_xgboost(X_train, y_train, X_val, y_val, df_val, test_data, out_dir):\n",
    "    print(\"\\n=== Training XGBoost ===\")\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=200, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=5, \n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        tree_method='hist',\n",
    "        device='cpu'\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # --- 验证集预测与保存 ---\n",
    "    val_probs = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # 保存 Validation CSV\n",
    "    save_to_csv(df_val, val_probs, out_dir, \"validation_set_xgb.csv\")\n",
    "    \n",
    "    val_dict = {\n",
    "        'sig': val_probs[y_val == 1],\n",
    "        'bkg': val_probs[y_val == 0]\n",
    "    }\n",
    "    \n",
    "    # --- 测试集预测与保存 ---\n",
    "    test_dict = {}\n",
    "    for name, (X_test, df_test) in test_data.items():\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "        test_dict[name] = probs\n",
    "        \n",
    "        # 保存 Test CSV\n",
    "        csv_name = f\"{name}_xgb.csv\"\n",
    "        save_to_csv(df_test, probs, out_dir, csv_name)\n",
    "        \n",
    "    plot_score_distributions(val_dict, test_dict, out_dir, model_name=\"XGBoost\")\n",
    "    return clf\n",
    "\n",
    "# --- MLP (PyTorch) ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def run_mlp(X_train, y_train, X_val, y_val, df_val, test_data, out_dir, device='cuda'):\n",
    "    print(\"\\n=== Training MLP ===\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train).unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    \n",
    "    model = SimpleMLP(in_dim=X_train.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    epochs = 30\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # --- 验证集预测与保存 ---\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "        val_probs = model(X_val_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # 保存 Validation CSV (注意：这里保存的是df_val原始值，不是scaler处理后的)\n",
    "    save_to_csv(df_val, val_probs, out_dir, \"validation_set_mlp.csv\")\n",
    "        \n",
    "    val_dict = {\n",
    "        'sig': val_probs[y_val == 1],\n",
    "        'bkg': val_probs[y_val == 0]\n",
    "    }\n",
    "    \n",
    "    # --- 测试集预测与保存 ---\n",
    "    test_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for name, (X_test, df_test) in test_data.items():\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            X_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "            probs = model(X_tensor).cpu().numpy().flatten()\n",
    "            test_dict[name] = probs\n",
    "            \n",
    "            # 保存 Test CSV\n",
    "            csv_name = f\"{name}_mlp.csv\"\n",
    "            save_to_csv(df_test, probs, out_dir, csv_name)\n",
    "            \n",
    "    plot_score_distributions(val_dict, test_dict, out_dir, model_name=\"MLP\")\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# Main Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 路径配置\n",
    "    sig_file = \"/home/zhonghua/data/Dataset_Filted/filted_Monopole_1e10_merged_1000_70_new_dataset_strict_300_500.npz\"\n",
    "    pr_file  = \"/home/zhonghua/data/Dataset_Filted/CosmicRay/Npz/Proton_1000_70_1e10_V03_dataset_strict_300_500.npz\"\n",
    "    \n",
    "    test_files = {\n",
    "        # \"EF_Model\": \"/home/zhonghua/data/Dataset_Filted/CosmicRay/Npz/0818_filted_EF_Proton_4e13_1e15_dataset_strict.npz\",\n",
    "        # \"QF_Model\": \"/home/zhonghua/data/Dataset_Filted/CosmicRay/Npz/0818_filted_QF_Proton_4e13_1e15_dataset_strict.npz\",\n",
    "        \"Exp_2022\": \"/home/zhonghua/data/Dataset_Filted/Experiment/2022/all_combined_2022_dataset_1e10_for_diffmodels_strict_300_500.npz\",\n",
    "        # \"QG_large\": \"/home/zhonghua/data/Dataset_Filted/CosmicRay/Npz/Proton_1000_70_1e10_V03_dataset_strict_300_500.npz\",\n",
    "        # \"Monopole_large\": \"/home/zhonghua/data/Dataset_Filted/filted_Monopole_1e10_merged_1000_70_new_dataset_strict_300_500.npz\",\n",
    "    }\n",
    "    \n",
    "    # 特征列表\n",
    "    feature_cols = ['R_mean', 'Eage','NuW2', 'rec_theta', 'rec_phi']\n",
    "    output_dir = \"/home/zhonghua/Filt_Event/model_validation/xgb_mlp_strict_300_500\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 设备配置\n",
    "    \n",
    "    device = \"cpu\" # 如果有GPU可改为 \"cuda\"\n",
    "    \n",
    "    # 1. 准备数据\n",
    "    print(\"Preparing data...\")\n",
    "    # 注意：prepare_data 现在返回 df_val 和 test_data (包含完整df)\n",
    "    X_train, y_train, X_val, y_val, df_val, test_data = prepare_data(\n",
    "        sig_file, pr_file, test_files, feature_cols\n",
    "    )\n",
    "    print(f\"Train size: {X_train.shape[0]}, Val size: {X_val.shape[0]}\")\n",
    "    \n",
    "    # 2. 运行 XGBoost (会自动保存CSV)\n",
    "    run_xgboost(X_train, y_train, X_val, y_val, df_val, test_data, output_dir)\n",
    "    \n",
    "    # # 3. 运行 MLP (会自动保存CSV)\n",
    "    # run_mlp(X_train, y_train, X_val, y_val, df_val, test_data, output_dir, device=device)\n",
    "    \n",
    "    print(\"\\nAll Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49750d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析参数\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import uproot\n",
    "# =====================================================\n",
    "# 1. IO 工具\n",
    "# =====================================================\n",
    "\n",
    "def read_csv_df(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    df=pd.read_csv(path)\n",
    "    df['weight']=np.ones(len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_val_scores_new(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    data= pd.read_csv(path)\n",
    "    sig_data=data[data['label']==43]\n",
    "    bkg_data=data[data['label']==14]\n",
    "    return sig_data, bkg_data\n",
    "\n",
    "\n",
    "def give_weights(params_df):\n",
    "    rootfile = \"/home/zhonghua/data/exposure.root\"\n",
    "    with uproot.open(rootfile) as f:\n",
    "        hWeight = f[\"hWeight\"]\n",
    "        edges_E = hWeight.axes[0].edges()  # Energy bin edges\n",
    "        edges_Zen = hWeight.axes[1].edges()  # Zenith bin edges\n",
    "        weights_2d = hWeight.values()\n",
    "\n",
    "    def add_weights(trueE, rec_zenith, weights_2d):\n",
    "        trueE = np.asarray(trueE, dtype=float)\n",
    "        trueE=np.log10(trueE/1e3)\n",
    "        rec_zenith=180/np.pi*rec_zenith\n",
    "        bin_idx_E = np.digitize(trueE, edges_E) - 1 \n",
    "        bin_idx_Zen = np.digitize(rec_zenith, edges_Zen) - 1\n",
    "        \n",
    "        return weights_2d[bin_idx_E, bin_idx_Zen] / 10 \n",
    "    trueE = params_df['trueE'].values\n",
    "    rec_zenith = params_df['theta'].values\n",
    "    weights = add_weights(trueE, rec_zenith, weights_2d)\n",
    "    params_df['weight'] = weights\n",
    "    return params_df\n",
    "\n",
    "def give_sig_weights(monopole_df):\n",
    "    file = uproot.open(\"/home/zhonghua/Filt_Event/model_validation/exposure.root\")\n",
    "    h1 = file[\"h1\"]\n",
    "    h2 = file[\"h2\"]\n",
    "    # 获取直方图的所有数据\n",
    "    values = h1.values()        # bin内容（纵坐标）\n",
    "    h2_values = h2.values()        # bin内容（纵坐标）\n",
    "    edges = h1.axis().edges()   # bin边界（横坐标）\n",
    "    h2_edges = h2.axis().edges()   # bin边界（横坐标）\n",
    "    centers = h1.axis().centers()  # bin中心\n",
    "    h2_centers = h2.axis().centers()  # bin中心\n",
    "\n",
    "    values = np.array(h1.values())\n",
    "    edges = np.array(h1.axis().edges())\n",
    "    centers = np.array(h1.axis().centers())\n",
    "    S=np.pi * 1000**2\n",
    "    parker=1e-15 * 1e4 \n",
    "    nums=[]\n",
    "    for i, theta in enumerate(centers):\n",
    "        cosine_theta = np.cos(np.deg2rad(theta))\n",
    "        area = S * cosine_theta\n",
    "        exposure = values[i] * area\n",
    "        num_i = exposure * parker\n",
    "        nums.append(num_i)\n",
    "\n",
    "    weights=[num_i/251000/h2_values[i]/86400 for i, num_i in enumerate(nums)]\n",
    "    theta_degrees = monopole_df[\"theta\"] * 180 / np.pi\n",
    "    monopole_weights = []\n",
    "    for theta in theta_degrees:\n",
    "        if theta > max(centers) or theta < 0:\n",
    "            weight = weights[-1] if theta > max(centers) else 0\n",
    "        else:\n",
    "            bin_id = np.digitize(theta, edges) - 1\n",
    "            weight = weights[bin_id]\n",
    "        monopole_weights.append(weight)\n",
    "\n",
    "    monopole_df[\"weight\"] = monopole_weights\n",
    "    return monopole_df\n",
    "\n",
    "# =====================================================\n",
    "# 2. 配置\n",
    "# =====================================================\n",
    "\n",
    "base_dir = \"/home/zhonghua/Filt_Event/model_validation/xgb_mlp\"\n",
    "\n",
    "EF_csv = f\"{base_dir}/EF_Model_xgb.csv\"\n",
    "QF_csv = f\"{base_dir}/QF_Model_xgb.csv\"\n",
    "QG_csv_large = f\"{base_dir}/QG_large_xgb.csv\"\n",
    "val_npz = f\"{base_dir}/validation_set_xgb.csv\"\n",
    "exp_2022_csv = f\"{base_dir}/Exp_2022_xgb.csv\"\n",
    "exp_2022_csv = f\"{base_dir}/Exp_2022_xgb.csv\"\n",
    "large_sig_csv = f\"{base_dir}/Monopole_large_xgb.csv\"\n",
    "\n",
    "# =====================================================\n",
    "# 3. 构建直方图\n",
    "# =====================================================\n",
    "# diff model EF+QF \n",
    "EF_scores  = read_csv_df(EF_csv)\n",
    "QF_scores  = read_csv_df(QF_csv)\n",
    "EF_scores = give_weights(EF_scores)\n",
    "QF_scores = give_weights(QF_scores)\n",
    "QG_scores_large  = read_csv_df(QG_csv_large)\n",
    "QG_scores_large = give_weights(QG_scores_large)\n",
    "# exp data\n",
    "exp_2022_scores = read_csv_df(exp_2022_csv)\n",
    "# val data\n",
    "sig_scores, bkg_scores = read_val_scores_new(val_npz)\n",
    "bkg_scores = give_weights(bkg_scores)\n",
    "# large sig score\n",
    "sig_scores = read_csv_df(large_sig_csv)\n",
    "sig_scores = give_sig_weights(sig_scores)\n",
    "# =====================================================\n",
    "# 4. 绘图\n",
    "# =====================================================\n",
    "bins=30\n",
    "# recE \n",
    "plt.hist(EF_scores[\"recE\"], weights=EF_scores[\"weight\"], bins=bins, histtype='step', label='EF', linestyle='--', density=True)\n",
    "plt.hist(QF_scores[\"recE\"], weights=QF_scores[\"weight\"], bins=bins, histtype='step', label='QF', linestyle='--', density=True)\n",
    "plt.hist(bkg_scores[\"recE\"], weights=bkg_scores[\"weight\"], bins=bins, histtype='step', label='QG', linestyle='--', density=True)\n",
    "plt.hist(QG_scores_large[\"recE\"], weights=QG_scores_large[\"weight\"], bins=bins, histtype='step', label='QG large', density=True)\n",
    "plt.hist(sig_scores[\"recE\"], weights=sig_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3, label='SIG', density=True)\n",
    "plt.hist(exp_2022_scores[\"recE\"], weights=exp_2022_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3,label='Exp', linestyle='--', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"recE\")\n",
    "plt.show()\n",
    "# rec_theta\n",
    "plt.hist(EF_scores[\"rec_theta\"], weights=EF_scores[\"weight\"], bins=bins, histtype='step', label='EF', linestyle='--', density=True)\n",
    "plt.hist(QF_scores[\"rec_theta\"], weights=QF_scores[\"weight\"], bins=bins, histtype='step', label='QF', linestyle='--', density=True)\n",
    "plt.hist(bkg_scores[\"rec_theta\"], weights=bkg_scores[\"weight\"], bins=bins, histtype='step', label='QG', linestyle='--', density=True)\n",
    "plt.hist(QG_scores_large[\"rec_theta\"], weights=QG_scores_large[\"weight\"], bins=bins, histtype='step', label='QG large', density=True)\n",
    "plt.hist(sig_scores[\"rec_theta\"], weights=sig_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3, label='SIG', density=True)\n",
    "plt.hist(exp_2022_scores[\"rec_theta\"], weights=exp_2022_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3,label='Exp', linestyle='--', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"rec_theta\")\n",
    "plt.show()\n",
    "# Eage\n",
    "plt.hist(EF_scores[\"Eage\"], weights=EF_scores[\"weight\"], bins=bins, histtype='step', label='EF', linestyle='--', density=True)\n",
    "plt.hist(QF_scores[\"Eage\"], weights=QF_scores[\"weight\"], bins=bins, histtype='step', label='QF', linestyle='--', density=True)\n",
    "plt.hist(bkg_scores[\"Eage\"], weights=bkg_scores[\"weight\"], bins=bins, histtype='step', label='QG', linestyle='--', density=True)\n",
    "plt.hist(QG_scores_large[\"Eage\"], weights=QG_scores_large[\"weight\"], bins=bins, histtype='step', label='QG large', density=True)\n",
    "plt.hist(sig_scores[\"Eage\"], weights=sig_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3, label='SIG', density=True)\n",
    "plt.hist(exp_2022_scores[\"Eage\"], weights=exp_2022_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3,label='Exp', linestyle='--', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Eage\")\n",
    "plt.show()\n",
    "# R_mean\n",
    "plt.hist(EF_scores[\"R_mean\"], weights=EF_scores[\"weight\"], bins=bins, histtype='step', label='EF', linestyle='--', density=True)\n",
    "plt.hist(QF_scores[\"R_mean\"], weights=QF_scores[\"weight\"], bins=bins, histtype='step', label='QF', linestyle='--', density=True)\n",
    "plt.hist(bkg_scores[\"R_mean\"], weights=bkg_scores[\"weight\"], bins=bins, histtype='step', label='QG', linestyle='--', density=True)\n",
    "plt.hist(QG_scores_large[\"R_mean\"], weights=QG_scores_large[\"weight\"], bins=bins, histtype='step', label='QG large', density=True)\n",
    "plt.hist(sig_scores[\"R_mean\"], weights=sig_scores[\"weight\"], bins=bins, histtype='stepfilled', alpha=0.3, label='SIG', density=True)\n",
    "plt.hist(exp_2022_scores[\"R_mean\"], weights=exp_2022_scores[\"weight\"], bins=bins, histtype='stepfilled',alpha=0.3, label='Exp', linestyle='--', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"R_mean\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
