import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GraphConv, global_mean_pool, global_max_pool
from torch_geometric.data import Data, Dataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GCNConv,GATConv, GlobalAttention, AttentionalAggregation

import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
from torch.cuda.amp import autocast, GradScaler
import os
from torch_geometric.nn import knn_graph, radius_graph
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
import pandas as pd

def hits_to_graph(hE, hM, only_ED=False, k=15, r=None):
    """
    hE: np.array [N_e, 3] (x, y, pe)
    hM: np.array [N_mu, 3] (x, y, pe)
    only_ED: æ˜¯å¦åªç”¨ç”µç£æŽ¢æµ‹å™¨
    k: kNN çš„é‚»å±…æ•°
    r: åŠå¾„é˜ˆå€¼ (è‹¥ç»™å®šåˆ™ç”¨ radius_graphï¼Œå¦åˆ™ç”¨ knn_graph)
    """
    if only_ED:
        hits = hE
    else:
        hits = np.concatenate([hE, hM], axis=0) if (len(hE) > 0 or len(hM) > 0) else np.zeros((0,3))

    if hits.shape[0] == 0:  
        # ç©ºäº‹ä»¶ â†’ è™šæ‹ŸèŠ‚ç‚¹
        hits = np.zeros((1, 3), dtype=np.float32)

    # åŸºç¡€ç‰¹å¾
    x, y, pe = hits[:,0], hits[:,1], hits[:,2]
    r2 = np.sqrt(x**2 + y**2)
    logpe = np.log1p(pe)  # log(1+pe)

    feats = np.stack([x, y, pe, logpe, r2], axis=1)  # [num_nodes, 5]
    x = torch.tensor(feats, dtype=torch.float32)

    # å»ºè¾¹
    if x.size(0) == 1:
        edge_index = torch.zeros((2,0), dtype=torch.long)  # æ²¡æœ‰è¾¹
    else:
        if r is not None:
            edge_index = radius_graph(x[:,:2], r, loop=False)  # ç”¨ (x,y) å»ºè¾¹
        else:
            edge_index = knn_graph(x[:,:2], k=k, loop=False)

    if edge_index.dim() == 1:
        edge_index = edge_index.unsqueeze(0)
    assert edge_index.size(0) == 2, f"edge_index wrong shape: {edge_index.shape}"

    
    return x, edge_index

class AirShowerGraphDataset(Dataset):
    def __init__(self, hitsE, hitsM, labels, only_ED=False, k=8, r=None):
        super().__init__()
        self.hitsE = hitsE
        self.hitsM = hitsM
        self.labels = labels
        self.only_ED = only_ED
        self.k = k
        self.r = r

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        hE = self.hitsE[idx]
        hM = self.hitsM[idx]
        label = int(self.labels[idx])

        x, edge_index = hits_to_graph(hE, hM, only_ED=self.only_ED, k=self.k, r=self.r)
        if edge_index.size(0) == 1:
            print(f"Graph idx={idx}, x={x.shape}, edge_index={edge_index.shape}")
        y = torch.tensor(label, dtype=torch.long)  # äº¤å‰ç†µè¦æ±‚æ ‡é‡ int
        return Data(x=x, edge_index=edge_index, y=y)
class AirShowerGraphDataset_new(Dataset):
    def __init__(self, hitsE, hitsM, labels,
                 csv_feats=None,
                 only_ED=False, k=15, r=None):
        super().__init__()
        self.hitsE = hitsE
        self.hitsM = hitsM
        self.labels = labels
        self.only_ED = only_ED
        self.k = k
        self.r = r
        self.csv_feats = csv_feats.astype(np.float32) if csv_feats is not None else None

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        hE = self.hitsE[idx]
        hM = self.hitsM[idx]
        label = int(self.labels[idx])

        x, edge_index = hits_to_graph(hE, hM, only_ED=self.only_ED, k=self.k, r=self.r)

        if self.csv_feats is not None:
            csv_feat = torch.tensor(self.csv_feats[idx], dtype=torch.float32)
            csv_feat = csv_feat.unsqueeze(0)  # å˜æˆ [1, 8]
        else:
            csv_feat = torch.zeros(0)

        y = torch.tensor(label, dtype=torch.long)
        return Data(x=x, edge_index=edge_index, y=y, csv_feat=csv_feat)

class GNNClassifier(nn.Module):
    def __init__(self, in_channels=6, hidden_channels=128, num_layers=3,
                 num_classes=2, dropout=0.3, heads=4):
        """
        in_channels: è¾“å…¥èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ (é»˜è®¤ 6: x,y,pe,logpe,r2,theta)
        hidden_channels: GNN éšè—å±‚ç»´åº¦
        num_layers: GAT å±‚æ•°
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        dropout: dropout æ¯”çŽ‡
        heads: GAT æ³¨æ„åŠ›å¤´æ•°
        """
        super().__init__()
        self.num_layers = num_layers
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()

        # ç¬¬ä¸€å±‚
        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, concat=False))
        self.bns.append(nn.BatchNorm1d(hidden_channels))

        # ä¸­é—´å±‚
        for _ in range(num_layers - 1):
            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=heads, concat=False))
            self.bns.append(nn.BatchNorm1d(hidden_channels))

        # æ³¨æ„åŠ›æ± åŒ–
        self.att_pool = AttentionalAggregation(
            gate_nn=nn.Sequential(
                nn.Linear(hidden_channels, hidden_channels // 2),
                nn.ReLU(),
                nn.Linear(hidden_channels // 2, 1)
            )
        )

        self.dropout = dropout

        # åˆ†ç±» MLP
        self.mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, num_classes)
        )

    def forward(self, x, edge_index, batch):
        for conv, bn in zip(self.convs, self.bns):
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # æ³¨æ„åŠ›æ± åŒ–
        x = self.att_pool(x, batch)

        # åˆ†ç±»
        out = self.mlp(x)
        return out

class GNNClassifier_new(nn.Module):
    def __init__(self, in_channels=6, hidden_channels=128, num_layers=3,
                 num_classes=2, dropout=0.3, heads=4, csv_dim=10):
        super().__init__()
        self.num_layers = num_layers
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()

        # GAT å±‚
        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, concat=False))
        self.bns.append(nn.BatchNorm1d(hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=heads, concat=False))
            self.bns.append(nn.BatchNorm1d(hidden_channels))

        # æ³¨æ„åŠ›æ± åŒ–
        self.att_pool = AttentionalAggregation(
            gate_nn=nn.Sequential(
                nn.Linear(hidden_channels, hidden_channels // 2),
                nn.ReLU(),
                nn.Linear(hidden_channels // 2, 1)
            )
        )

        self.dropout = dropout

        # MLP åˆ†ç±» (GNN è¡¨ç¤º + CSV ç‰¹å¾)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_channels + csv_dim, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, num_classes)
        )

    def forward(self, x, edge_index, batch, csv_feats):
        # GNN ç¼–ç 
        for conv, bn in zip(self.convs, self.bns):
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # å›¾æ± åŒ–
        gnn_out = self.att_pool(x, batch)

        # æ‹¼æŽ¥ event-level ç‰¹å¾
        out = torch.cat([gnn_out, csv_feats], dim=1)

        # åˆ†ç±»
        out = self.mlp(out)
        return out

def compute_avg_distance(x):
    # x: (N, d) èŠ‚ç‚¹ç‰¹å¾ï¼Œåªå–åæ ‡éƒ¨åˆ†ï¼Œæ¯”å¦‚å‰ä¸‰ç»´ (x,y,z) æˆ– (x,y)
    coords = x[:, :2]   # å¦‚æžœ x å‰ä¸‰åˆ—æ˜¯åæ ‡
    r = torch.cdist(coords, coords, p=2)
    avg_dist = r.mean().item()
    max_dist = r.max().item()
    print(f"å¹³å‡è·ç¦» = {avg_dist:.3f}, æœ€å¤§è·ç¦» = {max_dist:.3f}")
    return avg_dist, max_dist

# ----------------------------
# 2. è®­ç»ƒå‡½æ•°
# ----------------------------
def train(model, loader, optimizer,criterion, device, scaler=None, clip_grad=5.0, num_classes=None):
    model.train()
    total_loss = 0
    for batch_idx, data in enumerate(loader):
        data = data.to(device)
        # å¹³å‡è·ç¦» = 575.972, æœ€å¤§è·ç¦» = 4588.601
        # compute_avg_distance(data.x) 
        # print(data.num_nodes, data.num_edges)
        # ðŸ” Debug æ£€æŸ¥ edge_index æ˜¯å¦è¶Šç•Œ
        if data.edge_index.max().item() >= data.x.size(0):
            raise RuntimeError(
                f"[EdgeIndexError] batch={batch_idx}, "
                f"x={data.x.size()}, "
                f"edge_index max={data.edge_index.max().item()}"
            )

        # ðŸ” Debug æ£€æŸ¥ label æ˜¯å¦è¶Šç•Œ
        if num_classes is not None and data.y.max().item() >= num_classes:
            raise RuntimeError(
                f"[LabelError] batch={batch_idx}, "
                f"y_max={data.y.max().item()}, num_classes={num_classes}"
            )

        optimizer.zero_grad()

        with autocast(enabled=scaler is not None):
           
            out = model(data.x, data.edge_index, data.batch, data.csv_feat)
            # loss = F.cross_entropy(out, data.y)
            loss = criterion(out, data.y)

        if scaler:
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            optimizer.step()

        total_loss += loss.item()

    return total_loss / len(loader)

# ----------------------------
# 3. éªŒè¯å‡½æ•°
# ----------------------------
def evaluate(model, loader, device):
    model.eval()
    all_labels, all_probs = [], []
    correct, total = 0, 0
    val_loss = 0
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            out = model(data.x, data.edge_index, data.batch, data.csv_feat)
            loss = F.cross_entropy(out, data.y)
            val_loss += loss.item()
            probs = F.softmax(out, dim=1)[:,1]
            pred = out.argmax(dim=1)
            correct += (pred == data.y).sum().item()
            total += data.num_graphs
            all_labels.extend(data.y.cpu().tolist())
            all_probs.extend(probs.cpu().tolist())
    acc = correct / total
    labels_np = np.array(all_labels)
    probs_np = np.array(all_probs)
    mask = ~np.isnan(probs_np)
    auc = roc_auc_score(labels_np[mask], probs_np[mask]) if len(set(labels_np[mask])) > 1 else 0.0
    return val_loss / len(loader), acc, auc, labels_np[mask], probs_np[mask]

# ----------------------------
# 4. ä¸»è®­ç»ƒå¾ªçŽ¯
# ----------------------------
def main(data_file,csv_file,  model_file,version='1e9_V04'):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    data = np.load(data_file, allow_pickle=True)
    hitsE_all, hitsM_all, labels_all = data['hitsE'], data['hitsM'], data['labels']
    
    from sklearn.model_selection import train_test_split
    
    num_events = len(labels_all)
    indices = np.arange(num_events)

    train_idx, val_idx = train_test_split(indices, test_size=0.2,
                                        random_state=42, stratify=labels_all)

    # ç”¨ indices å–å­é›†
    hitsE_train, hitsM_train, labels_train = hitsE_all[train_idx], hitsM_all[train_idx], labels_all[train_idx]
    hitsE_val, hitsM_val, labels_val = hitsE_all[val_idx], hitsM_all[val_idx], labels_all[val_idx]

    # è¿™é‡Œä¹Ÿèƒ½ä¿è¯ CSV å¯¹åº”ï¼Œå› ä¸º CSV é‡Œç¬¬ i è¡Œå°±æ˜¯ event i
    csv_feats = pd.read_csv(csv_file)

    # åªé€‰å–éœ€è¦çš„åˆ—  'R_ue',
    params = [ 'R_mean', 'Eage', 'NpE1', 'NpE2']
    csv_feats = csv_feats[params].values.astype(np.float32)

    # æŒ‰ç…§åˆ’åˆ†å¥½çš„ index æ‹†åˆ† csv_feats
    train_csv = csv_feats[train_idx]
    val_csv   = csv_feats[val_idx]


    train_dataset = AirShowerGraphDataset_new(hitsE_train, hitsM_train, labels_train, csv_feats=train_csv, only_ED=True,k=15)
    val_dataset   = AirShowerGraphDataset_new(hitsE_val, hitsM_val, labels_val, csv_feats=val_csv, only_ED=True,k=15)

    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
    val_loader   = DataLoader(val_dataset, batch_size=256)
    print(f'âœ… Dataset loaded! Train: {len(train_dataset)}, Val: {len(val_dataset)}, Total: {len(labels_all)}')

   
    # model = GNNClassifier(in_channels=5).to(device)
    model = GNNClassifier_new(in_channels=5, csv_dim=len(params)).to(device)
   
    # åŠ è½½æ¨¡åž‹
    best_loss, best_acc, best_auc = 1, 0, 0
    if os.path.exists(model_file):
        model.load_state_dict(torch.load(model_file))
        print(f"âœ… Model loaded from {model_file}")
        best_loss, best_acc, best_auc, _, _ =evaluate(model, val_loader, device)
        print(f"Best model loaded from {model_file} | Best Val Loss: {best_loss:.4f} | Best Val Acc: {best_acc:.4f} | Best Val AUC: {best_auc:.4f}")

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=len(train_loader), T_mult=1, eta_min=1e-7
    )
    weight = torch.tensor([1.0, 1],device=device)  # èƒŒæ™¯=1, signal=9å€æƒé‡
    criterion = nn.CrossEntropyLoss(weight=weight)
    scaler = GradScaler()

    patience, epochs_no_improve =20, 0
    num_epochs = 50
    logs = {"train_loss": [], "val_loss": [], "val_acc": [], "val_auc": []}

    for epoch in range(num_epochs):
        train_loss = train(model, train_loader, optimizer, criterion, device, scaler=scaler, num_classes=2)
        val_loss, val_acc, val_auc, val_labels, val_probs = evaluate(model, val_loader, device)
        scheduler.step()

        logs["train_loss"].append(train_loss)
        logs["val_loss"].append(val_loss)
        logs["val_acc"].append(val_acc)
        logs["val_auc"].append(val_auc)

        print(f"Epoch {epoch+1}/{num_epochs} | "
              f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
              f"Val Acc: {val_acc:.4f} | Val AUC: {val_auc:.4f}")
        bkg_index = np.where(val_labels==0)[0]
        sig_index = np.where(val_labels==1)[0]
        thresholds=[0.8, 0.9, 0.95, 0.99]
        bkg_rate=len(np.where(val_probs[bkg_index]>0.9)[0])/len(bkg_index)
        sig_rate=len(np.where(val_probs[sig_index]>0.9)[0])/len(sig_index)
        for threshold in thresholds:
            print(f'    when threshold={threshold:.2f}: bkg rate={len(np.where(val_probs[bkg_index]>threshold)[0])/len(bkg_index):.4f}, sig rate={len(np.where(val_probs[sig_index]>threshold)[0])/len(sig_index):.4f}')
        signif=sig_rate/np.sqrt(bkg_rate+1e-10)
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), model_file)
            print(f"âœ… New best model saved! Val loss: {val_loss:.4f}")
            
            plt.hist(val_probs[bkg_index], bins=50, alpha=0.5, density=True, label='Background')
            plt.hist(val_probs[sig_index], bins=50, alpha=0.5, density=True, label='Signal')
            plt.xlim(0, 1)
            plt.legend()
            plt.savefig(f'/home/zhonghua/Filt_Event/figures/GNN_best_model_{version}_csv.png')
            plt.close()
            epochs_no_improve = 0
            
            np.savez(f"/home/zhonghua/Filt_Event/figures/GNN_Val_hist_{version}_csv.npz", bkg_hist=val_probs[bkg_index], sig_hist=val_probs[sig_index])
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience:
            print(f"ðŸš¨ Early stopping triggered at epoch {epoch+1}")
            break

    fig, axs = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦è¾¹ç”» Loss æ›²çº¿
    axs[0].plot(logs["train_loss"], label="Train Loss")
    axs[0].plot(logs["val_loss"], label="Val Loss")
    axs[0].set_xlabel("Epoch")
    axs[0].set_ylabel("Loss")
    axs[0].legend()
    axs[0].set_title("Training vs Validation Loss")

    # å³è¾¹ç”» Val Acc å’Œ Val AUC
    axs[1].plot(logs["val_acc"], label="Val Acc")
    axs[1].plot(logs["val_auc"], label="Val AUC")
    axs[1].set_xlabel("Epoch")
    axs[1].set_ylabel("Score")
    axs[1].legend()
    axs[1].set_title("Validation Metrics")

    plt.tight_layout()
    plt.savefig(f"/home/zhonghua/Filt_Event/figures/GNN_training_summary_{version}_csv.png")
    plt.close()


def test_model(data_file,csv_file, model_file, version):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    data = np.load(data_file, allow_pickle=True)
    hitsE_val, hitsM_val, labels_val = data['hitsE'], data['hitsM'], data['labels']
 
    csv_feats = pd.read_csv(csv_file)

    # åªé€‰å–éœ€è¦çš„åˆ—  'R_ue',
    params = [ 'R_mean', 'Eage', 'NpE1', 'NpE2']
    csv_feats = csv_feats[params].values.astype(np.float32)


    val_dataset   = AirShowerGraphDataset_new(hitsE_val, hitsM_val, labels_val, csv_feats=csv_feats, only_ED=True,k=15)
    val_loader   = DataLoader(val_dataset, batch_size=256)
    print(f'âœ… Dataset loaded!  Val: {len(val_dataset)}')


    model = GNNClassifier_new(in_channels=5, csv_dim=len(params)).to(device)

    if os.path.exists(model_file):
        model.load_state_dict(torch.load(model_file))
        print(f"âœ… Model loaded from {model_file}")
        best_loss, best_acc, best_auc, labels, probs =evaluate(model, val_loader, device)
        print(f"test:  Best model loaded from {model_file} | Best Val Loss: {best_loss:.4f} | Best Val Acc: {best_acc:.4f} | Best Val AUC: {best_auc:.4f}")
        bkg_index = np.where(labels==0)[0]
        sig_index = np.where(labels==1)[0]
        plt.hist(probs[bkg_index], bins=50, alpha=0.5, density=True, label='Background')
        plt.hist(probs[sig_index], bins=50, alpha=0.5, density=True, label='Signal')
        plt.xlim(0, 1)
        plt.legend()
        plt.savefig(f'/home/zhonghua/Filt_Event/figures/GNN_test_model_{version}-csv.png')
        plt.close()
        thresholds=[0.8, 0.9, 0.95, 0.99]
        for threshold in thresholds:    
            print(f'    when threshold={threshold:.2f}: bkg rate={len(np.where(probs[bkg_index]>threshold)[0])/len(bkg_index):.4f}, sig rate={len(np.where(probs[sig_index]>threshold)[0])/len(sig_index):.4f}')

        np.savez(f"/home/zhonghua/Filt_Event/figures/GNN_Test_hist_{version}.npz", bkg_hist=probs[bkg_index], sig_hist=probs[sig_index])
        

if __name__ == "__main__":
    version='1e11_V03'
    data_file=f"/data/zhonghua/Dataset_Filted/ForTrain/combined_filted_{version}_dataset.npz"
    csv_file = f"/data/zhonghua/Dataset_Filted/ForTrain/combined_filted_{version}_params.csv"
    model_file = f'/home/zhonghua/Filt_Event/models/GNN_best_model_{version}_csv.pth'
    
    main(data_file, csv_file, model_file, version)

    # test_data_file=f"/data/zhonghua/Dataset_Filted/ForTrain/TEST_combined_filted_{version}_dataset.npz"
    data_file="/data/zhonghua/Dataset_Filted/ForTrain/TEST_combined_filted_1e11_V03_dataset.npz"
    # model_file = f'/home/zhonghua/Filt_Event/models/GNN_best_model_{version}_csv-gamma.pth'
    csv_file = "/data/zhonghua/Dataset_Filted/ForTrain/TEST_combined_filted_1e11_V03_params.csv"
    test_model(data_file,csv_file, model_file, version)