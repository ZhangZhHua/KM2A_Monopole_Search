{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68863a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集1: (79075106, 22)\n",
      "数据集2: (2184367, 23)\n",
      "数据集3: (1456245, 23)\n",
      "Index(['mjd', 'r_Theta', 'r_Phi', 'r_Corex', 'r_Corey', 'NhitE', 'NfiltE',\n",
      "       'NtrigE', 'NhitM', 'NfiltM', 'NpE1', 'NpE2', 'NpE3', 'NuM1', 'NuM2',\n",
      "       'NuM3', 'size', 'age', 'dr', 'NuW2', 'recE', 'R_ue'],\n",
      "      dtype='object')\n",
      "Index(['mjd', 'r_Theta', 'r_Phi', 'r_Corex', 'r_Corey', 'NhitE', 'NfiltE',\n",
      "       'NtrigE', 'NhitM', 'NfiltM', 'NpE1', 'NpE2', 'NpE3', 'NuM1', 'NuM2',\n",
      "       'NuM3', 'size', 'age', 'dr', 'NuW2', 'recE', 'weight', 'R_ue'],\n",
      "      dtype='object')\n",
      "数据集1: (1000000, 22)\n",
      "数据集2: (1000000, 23)\n",
      "数据集3: (1000000, 23)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import uproot\n",
    "\n",
    "def read_root_to_pd(file_path):\n",
    "    \"\"\"读取ROOT文件中的ntuple数据到pandas DataFrame\"\"\"\n",
    "    file = uproot.open(file_path)\n",
    "    ntuple = file[\"ntuple\"]\n",
    "    return ntuple.arrays(library=\"pd\")\n",
    "\n",
    "# 读取两个文件\n",
    "exp_df = read_root_to_pd(\"/home/zhonghua/data/Dataset_Filted/AExp_params_6m5fpd.root\")\n",
    "mc_df = read_root_to_pd(\"/home/zhonghua/data/Dataset_Filted/Proton_train_60.root\")  # train data to give weight\n",
    "mc_test_df = read_root_to_pd(\"/home/zhonghua/data/Dataset_Filted/Proton_closure_40.root\") # closure test data\n",
    "\n",
    "\n",
    "# 计算R_ue\n",
    "ratio_exp = (exp_df[\"NuM1\"] + 1e-4) / (exp_df[\"NpE3\"] + 1)\n",
    "ratio_mc = (mc_df[\"NuM1\"] + 1e-4) / (mc_df[\"NpE3\"] + 1)\n",
    "ratio_mc_test = (mc_test_df[\"NuM1\"] + 1e-4) / (mc_test_df[\"NpE3\"] + 1)\n",
    "exp_df[\"R_ue\"] = np.log10(np.clip(ratio_exp, 1e-8, None))  # 避免log(0)\n",
    "mc_df[\"R_ue\"] = np.log10(np.clip(ratio_mc, 1e-8, None))\n",
    "mc_test_df[\"R_ue\"] = np.log10(np.clip(ratio_mc_test, 1e-8, None))\n",
    "\n",
    "print(f\"数据集1: {exp_df.shape}\")\n",
    "print(f\"数据集2: {mc_df.shape}\")\n",
    "print(f\"数据集3: {mc_test_df.shape}\")\n",
    "print(exp_df.columns)\n",
    "print(mc_df.columns)\n",
    "\n",
    "sample_num= 1e6\n",
    "exp_df_sample = exp_df.sample(n=int(sample_num))\n",
    "mc_df_sample = mc_df.sample(n=int(sample_num))\n",
    "mc_test_df_sample = mc_test_df.sample(n=int(sample_num))\n",
    "# 释放原始df\n",
    "del exp_df, mc_df, mc_test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4444184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reweighter on: (200000, 3) exp vs (200000, 3) mc (weights sum 9131765.8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    149\u001b[39m     w_mc_train_small = w_mc_train\n\u001b[32m    151\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining reweighter on:\u001b[39m\u001b[33m\"\u001b[39m, X_exp_train.shape, \u001b[33m\"\u001b[39m\u001b[33mexp vs\u001b[39m\u001b[33m\"\u001b[39m, X_mc_train_small.shape, \u001b[33m\"\u001b[39m\u001b[33mmc (weights sum \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m.format(np.sum(w_mc_train_small)))\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m model, clf_base = train_reweighter(X_mc_train_small, X_exp_train, w_mc_train_small)\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# apply to test set\u001b[39;00m\n\u001b[32m    156\u001b[39m X_mc_test = mc_test_df[vars_features].values[mask_test] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmask_test\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m mc_test_df[vars_features].values\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_reweighter\u001b[39m\u001b[34m(X_mc_train, X_exp_train, w_mc_train)\u001b[39m\n\u001b[32m     66\u001b[39m clf.fit(X_tr, y_tr, sample_weight=sw_tr)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# calibrate probabilities on validation set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m calibrator = CalibratedClassifierCV(base_estimator=clf, method=calibration_method, cv=\u001b[33m'\u001b[39m\u001b[33mprefit\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     70\u001b[39m calibrator.fit(X_val, y_val, sample_weight=sw_val)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m calibrator, clf\n",
      "\u001b[31mTypeError\u001b[39m: CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# ---------- 配置区（按需修改） ----------\n",
    "vars_features = [\"r_Theta\", \"age\", \"dr\"]   # 用于 reweight 的特征（control region）\n",
    "label_exp = 1\n",
    "label_mc = 0\n",
    "\n",
    "# Classifier 超参（可调整）\n",
    "clf_args = dict(n_estimators=300, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "\n",
    "# Calibration method: 'isotonic' or 'sigmoid'\n",
    "calibration_method = 'isotonic'\n",
    "\n",
    "# 权重剪切与归一化参数\n",
    "clip_min = 1e-3\n",
    "clip_max = 50.0   # 上限，防止单事件主导\n",
    "normalize_mc_total = True   # 将 reweighted MC 保持与原始 MC total weight 相同（常用）\n",
    "\n",
    "# bootstrap 设置\n",
    "Nboot = 300\n",
    "random_seed = 12345\n",
    "n_jobs = 4   # 若使用并行训练的模型可设置\n",
    "\n",
    "# closure 测试相关\n",
    "closure_var = \"recE\"\n",
    "rue_edges = np.linspace(np.percentile(exp_df[closure_var], 1),\n",
    "                        np.percentile(exp_df[closure_var], 99), 50)\n",
    "cut_low, cut_high = 1.79, 2.13\n",
    "# ----------------------------------------\n",
    "\n",
    "rng_global = np.random.default_rng(random_seed)\n",
    "\n",
    "# ---------- 辅助函数 ----------\n",
    "def train_reweighter(X_mc_train, X_exp_train, w_mc_train=None):\n",
    "    \"\"\"\n",
    "    Train classifier to separate data vs mc.\n",
    "    Inputs:\n",
    "      X_mc_train: (N_mc_train, n_feat) numpy array\n",
    "      X_exp_train: (N_exp_train, n_feat)\n",
    "      w_mc_train: original mc weights for training (or None -> ones)\n",
    "    Returns:\n",
    "      calibrator: calibrated classifier object with predict_proba\n",
    "      clf_base: raw classifier (optional)\n",
    "    \"\"\"\n",
    "    X = np.vstack([X_exp_train, X_mc_train])\n",
    "    y = np.concatenate([np.ones(len(X_exp_train)), np.zeros(len(X_mc_train))])\n",
    "    # sample weights: data weight = 1, mc weight = w_mc_train (if provided)\n",
    "    if w_mc_train is None:\n",
    "        sw = np.concatenate([np.ones(len(X_exp_train)), np.ones(len(X_mc_train))])\n",
    "    else:\n",
    "        sw = np.concatenate([np.ones(len(X_exp_train)), w_mc_train])\n",
    "\n",
    "    # train-test split for calibration set\n",
    "    X_tr, X_val, y_tr, y_val, sw_tr, sw_val = train_test_split(\n",
    "        X, y, sw, test_size=0.25, random_state=random_seed, stratify=y)\n",
    "\n",
    "    clf = GradientBoostingClassifier(**clf_args)\n",
    "    clf.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
    "\n",
    "    # calibrate probabilities on validation set\n",
    "    calibrator = CalibratedClassifierCV(base_estimator=clf, method=calibration_method, cv='prefit')\n",
    "    calibrator.fit(X_val, y_val, sample_weight=sw_val)\n",
    "    return calibrator, clf\n",
    "\n",
    "def get_reweight_from_model(model, X_mc_eval, w_mc_orig=None, clip=(clip_min, clip_max), normalize=normalize_mc_total):\n",
    "    \"\"\"\n",
    "    Compute event-wise reweight for MC eval set.\n",
    "    reweight = p/(1-p) where p = model.predict_proba(X)[:,1]\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(X_mc_eval)[:,1]\n",
    "    eps = 1e-6\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    w_rew = p / (1.0 - p)\n",
    "\n",
    "    # optional: multiply by original mc weight if provided (we keep them separate)\n",
    "    if w_mc_orig is None:\n",
    "        w = w_rew\n",
    "    else:\n",
    "        w = w_rew * w_mc_orig\n",
    "\n",
    "    # clip extreme values\n",
    "    w = np.clip(w, clip[0], clip[1])\n",
    "\n",
    "    if normalize and (w_mc_orig is not None):\n",
    "        # keep total MC weight same as original total (shape correction only)\n",
    "        total_orig = np.sum(w_mc_orig)\n",
    "        total_new = np.sum(w)\n",
    "        if total_new > 0:\n",
    "            w = w * (total_orig / total_new)\n",
    "    return w, w_rew  # return both final applied weights and raw reweight\n",
    "\n",
    "def weighted_hist_and_uncertainty(vals, weights, bins):\n",
    "    \"\"\"\n",
    "    Fast weighted histogram + per-bin sum of squared weights (for variance)\n",
    "    Returns:\n",
    "      hist (sum weights per bin), w2_per_bin (sum weights^2)\n",
    "    \"\"\"\n",
    "    bin_idx = np.digitize(vals, bins) - 1\n",
    "    nbins = len(bins) - 1\n",
    "    bin_idx = np.clip(bin_idx, 0, nbins-1)\n",
    "    hist = np.bincount(bin_idx, weights=weights, minlength=nbins)\n",
    "    w2 = np.bincount(bin_idx, weights=weights**2, minlength=nbins)\n",
    "    return hist, w2\n",
    "\n",
    "def chi2_between_histograms(data_hist, mc_hist, sigma_data=None, sigma_mc=None):\n",
    "    \"\"\"\n",
    "    Compute chi2 = sum ((D - M)^2 / (sigma_D^2 + sigma_M^2))\n",
    "    Provide sigma arrays or compute default (sqrt for data, sqrt(w2) for mc)\n",
    "    \"\"\"\n",
    "    if sigma_data is None:\n",
    "        sigma_data = np.sqrt(data_hist)\n",
    "    if sigma_mc is None:\n",
    "        # caller should provide w2->sqrt(w2) if needed\n",
    "        raise ValueError(\"Please provide sigma_mc (sqrt of sum w^2 per bin).\")\n",
    "    denom = sigma_data**2 + sigma_mc**2 + 1e-12\n",
    "    valid = denom > 0\n",
    "    chi2 = np.sum(((data_hist[valid] - mc_hist[valid])**2) / denom[valid])\n",
    "    return chi2\n",
    "\n",
    "# ---------- Nominal training + closure (single run) ----------\n",
    "# Build training arrays (control region selection: be careful to choose CR!)\n",
    "# Here I assume exp_df and mc_df are already representing the control region.\n",
    "X_exp = exp_df[vars_features].values\n",
    "X_mc_train = mc_df[vars_features].values\n",
    "w_mc_train = mc_df[\"weight\"].values  # original mc weight\n",
    "\n",
    "# optional: downsample if extremely large for speed (but keep sufficient statistics)\n",
    "max_train = 200000\n",
    "if len(X_exp) > max_train:\n",
    "    idx = rng_global.choice(len(X_exp), max_train, replace=False)\n",
    "    X_exp_train = X_exp[idx]\n",
    "else:\n",
    "    X_exp_train = X_exp\n",
    "\n",
    "if len(X_mc_train) > max_train:\n",
    "    idx2 = rng_global.choice(len(X_mc_train), max_train, replace=False)\n",
    "    X_mc_train_small = X_mc_train[idx2]\n",
    "    w_mc_train_small = w_mc_train[idx2]\n",
    "else:\n",
    "    X_mc_train_small = X_mc_train\n",
    "    w_mc_train_small = w_mc_train\n",
    "\n",
    "print(\"Training reweighter on:\", X_exp_train.shape, \"exp vs\", X_mc_train_small.shape, \"mc (weights sum {:.1f})\".format(np.sum(w_mc_train_small)))\n",
    "\n",
    "model, clf_base = train_reweighter(X_mc_train_small, X_exp_train, w_mc_train_small)\n",
    "\n",
    "# apply to test set\n",
    "X_mc_test = mc_test_df[vars_features].values[mask_test] if 'mask_test' in globals() else mc_test_df[vars_features].values\n",
    "w_mc_test_orig = mc_test_df[\"weight\"].values[mask_test] if 'mask_test' in globals() else mc_test_df[\"weight\"].values\n",
    "w_test_new, w_test_raw = get_reweight_from_model(model, X_mc_test, w_mc_test_orig, clip=(clip_min, clip_max))\n",
    "\n",
    "# compute histograms and chi2\n",
    "data_hist, _ = np.histogram(exp_df[closure_var].values, bins=rue_edges)\n",
    "mc_hist_rew, w2 = weighted_hist_and_uncertainty(mc_test_df[closure_var].values[mask_test], w_test_new, rue_edges)\n",
    "sigma_data = np.sqrt(data_hist)\n",
    "sigma_mc_rew = np.sqrt(w2)\n",
    "chi2_val = chi2_between_histograms(data_hist, mc_hist_rew, sigma_data=sigma_data, sigma_mc=sigma_mc_rew)\n",
    "ndf = len(data_hist) - 1\n",
    "pval = chi2.sf(chi2_val, ndf)\n",
    "print(\"Nominal classifier reweight closure: chi2/ndf = {:.2f}/{}, p-value {:.3g}\".format(chi2_val, ndf, pval))\n",
    "\n",
    "# efficiency in interval\n",
    "mask_data = (exp_df[closure_var].values > cut_low) & (exp_df[closure_var].values < cut_high)\n",
    "eps_data = np.sum(mask_data) / np.sum(np.isfinite(exp_df[closure_var].values))\n",
    "mask_test_cut = (mc_test_df[closure_var].values[mask_test] > cut_low) & (mc_test_df[closure_var].values[mask_test] < cut_high)\n",
    "eps_mc_test_rew = np.sum(w_test_new[mask_test_cut]) / np.sum(w_test_new)\n",
    "print(\"eff_data = {:.5f}, eff_mc_test_rew = {:.5f}, ratio MC/Data = {:.3f}\".format(eps_data, eps_mc_test_rew, eps_mc_test_rew/eps_data))\n",
    "\n",
    "# plot comparison\n",
    "plt.figure(figsize=(8,4))\n",
    "bin_centers = 0.5*(rue_edges[:-1] + rue_edges[1:])\n",
    "plt.step(bin_centers, data_hist, where='mid', label='exp (counts)')\n",
    "plt.step(bin_centers, mc_hist_rew, where='mid', label='mc_rew (weighted counts)')\n",
    "plt.legend()\n",
    "plt.ylabel(\"counts\")\n",
    "plt.xlabel(closure_var)\n",
    "plt.title(\"Nominal classifier reweight closure\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- Bootstrap (index bootstrap for train sets) ----------\n",
    "chi2s = []\n",
    "effs = []\n",
    "raw_weights_records = []  # optional if you want to inspect distribution of raw reweights (memory heavy)\n",
    "\n",
    "for i in tqdm(range(Nboot), desc=\"bootstrap reweights\"):\n",
    "    # resample indices with replacement for training sets\n",
    "    idx_exp_b = rng_global.integers(0, len(X_exp), size=len(X_exp))\n",
    "    idx_mc_b = rng_global.integers(0, len(X_mc_train), size=len(X_mc_train))\n",
    "    X_exp_b = X_exp[idx_exp_b]\n",
    "    X_mc_b = X_mc_train[idx_mc_b]\n",
    "    w_mc_b = w_mc_train[idx_mc_b]\n",
    "\n",
    "    # train reweighter on bootstraped samples (we use smaller model to speed if needed)\n",
    "    try:\n",
    "        model_b, _ = train_reweighter(X_mc_b, X_exp_b, w_mc_b)\n",
    "    except Exception as e:\n",
    "        # fallback: skip this bootstrap\n",
    "        continue\n",
    "\n",
    "    # apply to test\n",
    "    w_test_b, _ = get_reweight_from_model(model_b, X_mc_test, w_mc_test_orig, clip=(clip_min, clip_max))\n",
    "    # hist and chi2\n",
    "    mc_hist_b, w2_b = weighted_hist_and_uncertainty(mc_test_df[closure_var].values[mask_test], w_test_b, rue_edges)\n",
    "    sigma_mc_b = np.sqrt(w2_b)\n",
    "    chi2_b = chi2_between_histograms(data_hist, mc_hist_b, sigma_data=sigma_data, sigma_mc=sigma_mc_b)\n",
    "    chi2s.append(chi2_b)\n",
    "    # efficiency\n",
    "    eff_b = np.sum(w_test_b[mask_test_cut]) / np.sum(w_test_b)\n",
    "    effs.append(eff_b)\n",
    "\n",
    "chi2s = np.array(chi2s)\n",
    "effs = np.array(effs)\n",
    "print(\"Bootstrap chi2 median {:.2f}, 16-84 pct: {:.2f} - {:.2f}\".format(np.median(chi2s), np.percentile(chi2s,16), np.percentile(chi2s,84)))\n",
    "print(\"Bootstrap eff median {:.5f}, 16-84 pct: {:.5f} - {:.5f}\".format(np.median(effs), np.percentile(effs,16), np.percentile(effs,84)))\n",
    "\n",
    "# Plot closure residual distribution for efficiency\n",
    "deltas = eps_data - effs\n",
    "plt.figure()\n",
    "plt.hist(deltas, bins=40, density=True, histtype='step', color='red')\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.xlabel(\"closure residual (eps_data - eps_mc_rew)\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.title(\"bootstrap closure residuals\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- Compare to histogram-based nominal (if you have nominal histogram wmap) ----------\n",
    "# If you computed histogram-based MC weights earlier (nominal_wmap), show overlay for recE:\n",
    "# compute mc_hist_histnom (apply nominal_wmap via bin indices precomputed similarly)\n",
    "# (left as an exercise; use weighted_hist_and_uncertainty with weights from hist-based pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# ---------- 配置区（按需修改） ----------\n",
    "vars_features = [\"r_Theta\", \"age\", \"dr\"]   # 用于 reweight 的特征（control region）\n",
    "label_exp = 1\n",
    "label_mc = 0\n",
    "\n",
    "# Classifier 超参（可调整）\n",
    "clf_args = dict(n_estimators=300, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "\n",
    "# Calibration method: 'isotonic' or 'sigmoid'\n",
    "calibration_method = 'isotonic'\n",
    "\n",
    "# 权重剪切与归一化参数\n",
    "clip_min = 1e-3\n",
    "clip_max = 50.0   # 上限，防止单事件主导\n",
    "normalize_mc_total = True   # 将 reweighted MC 保持与原始 MC total weight 相同（常用）\n",
    "\n",
    "# bootstrap 设置\n",
    "Nboot = 300\n",
    "random_seed = 12345\n",
    "n_jobs = 4   # 若使用并行训练的模型可设置\n",
    "\n",
    "# closure 测试相关\n",
    "closure_var = \"recE\"\n",
    "rue_edges = np.linspace(np.percentile(exp_df[closure_var], 1),\n",
    "                        np.percentile(exp_df[closure_var], 99), 50)\n",
    "cut_low, cut_high = 1.79, 2.13\n",
    "# ----------------------------------------\n",
    "\n",
    "rng_global = np.random.default_rng(random_seed)\n",
    "\n",
    "# ---------- 辅助函数 ----------\n",
    "def train_reweighter(X_mc_train, X_exp_train, w_mc_train=None):\n",
    "    \"\"\"\n",
    "    Train classifier to separate data vs mc.\n",
    "    Inputs:\n",
    "      X_mc_train: (N_mc_train, n_feat) numpy array\n",
    "      X_exp_train: (N_exp_train, n_feat)\n",
    "      w_mc_train: original mc weights for training (or None -> ones)\n",
    "    Returns:\n",
    "      calibrator: calibrated classifier object with predict_proba\n",
    "      clf_base: raw classifier (optional)\n",
    "    \"\"\"\n",
    "    X = np.vstack([X_exp_train, X_mc_train])\n",
    "    y = np.concatenate([np.ones(len(X_exp_train)), np.zeros(len(X_mc_train))])\n",
    "    # sample weights: data weight = 1, mc weight = w_mc_train (if provided)\n",
    "    if w_mc_train is None:\n",
    "        sw = np.concatenate([np.ones(len(X_exp_train)), np.ones(len(X_mc_train))])\n",
    "    else:\n",
    "        sw = np.concatenate([np.ones(len(X_exp_train)), w_mc_train])\n",
    "\n",
    "    # train-test split for calibration set\n",
    "    X_tr, X_val, y_tr, y_val, sw_tr, sw_val = train_test_split(\n",
    "        X, y, sw, test_size=0.25, random_state=random_seed, stratify=y)\n",
    "\n",
    "    clf = GradientBoostingClassifier(**clf_args)\n",
    "    clf.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
    "\n",
    "    # calibrate probabilities on validation set\n",
    "    calibrator = CalibratedClassifierCV(base_estimator=clf, method=calibration_method, cv='prefit')\n",
    "    calibrator.fit(X_val, y_val, sample_weight=sw_val)\n",
    "    return calibrator, clf\n",
    "\n",
    "def get_reweight_from_model(model, X_mc_eval, w_mc_orig=None, clip=(clip_min, clip_max), normalize=normalize_mc_total):\n",
    "    \"\"\"\n",
    "    Compute event-wise reweight for MC eval set.\n",
    "    reweight = p/(1-p) where p = model.predict_proba(X)[:,1]\n",
    "    \"\"\"\n",
    "    p = model.predict_proba(X_mc_eval)[:,1]\n",
    "    eps = 1e-6\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    w_rew = p / (1.0 - p)\n",
    "\n",
    "    # optional: multiply by original mc weight if provided (we keep them separate)\n",
    "    if w_mc_orig is None:\n",
    "        w = w_rew\n",
    "    else:\n",
    "        w = w_rew * w_mc_orig\n",
    "\n",
    "    # clip extreme values\n",
    "    w = np.clip(w, clip[0], clip[1])\n",
    "\n",
    "    if normalize and (w_mc_orig is not None):\n",
    "        # keep total MC weight same as original total (shape correction only)\n",
    "        total_orig = np.sum(w_mc_orig)\n",
    "        total_new = np.sum(w)\n",
    "        if total_new > 0:\n",
    "            w = w * (total_orig / total_new)\n",
    "    return w, w_rew  # return both final applied weights and raw reweight\n",
    "\n",
    "def weighted_hist_and_uncertainty(vals, weights, bins):\n",
    "    \"\"\"\n",
    "    Fast weighted histogram + per-bin sum of squared weights (for variance)\n",
    "    Returns:\n",
    "      hist (sum weights per bin), w2_per_bin (sum weights^2)\n",
    "    \"\"\"\n",
    "    bin_idx = np.digitize(vals, bins) - 1\n",
    "    nbins = len(bins) - 1\n",
    "    bin_idx = np.clip(bin_idx, 0, nbins-1)\n",
    "    hist = np.bincount(bin_idx, weights=weights, minlength=nbins)\n",
    "    w2 = np.bincount(bin_idx, weights=weights**2, minlength=nbins)\n",
    "    return hist, w2\n",
    "\n",
    "def chi2_between_histograms(data_hist, mc_hist, sigma_data=None, sigma_mc=None):\n",
    "    \"\"\"\n",
    "    Compute chi2 = sum ((D - M)^2 / (sigma_D^2 + sigma_M^2))\n",
    "    Provide sigma arrays or compute default (sqrt for data, sqrt(w2) for mc)\n",
    "    \"\"\"\n",
    "    if sigma_data is None:\n",
    "        sigma_data = np.sqrt(data_hist)\n",
    "    if sigma_mc is None:\n",
    "        # caller should provide w2->sqrt(w2) if needed\n",
    "        raise ValueError(\"Please provide sigma_mc (sqrt of sum w^2 per bin).\")\n",
    "    denom = sigma_data**2 + sigma_mc**2 + 1e-12\n",
    "    valid = denom > 0\n",
    "    chi2 = np.sum(((data_hist[valid] - mc_hist[valid])**2) / denom[valid])\n",
    "    return chi2\n",
    "\n",
    "# ---------- Nominal training + closure (single run) ----------\n",
    "# Build training arrays (control region selection: be careful to choose CR!)\n",
    "# Here I assume exp_df and mc_df are already representing the control region.\n",
    "X_exp = exp_df[vars_features].values\n",
    "X_mc_train = mc_df[vars_features].values\n",
    "w_mc_train = mc_df[\"weight\"].values  # original mc weight\n",
    "\n",
    "# optional: downsample if extremely large for speed (but keep sufficient statistics)\n",
    "max_train = 200000\n",
    "if len(X_exp) > max_train:\n",
    "    idx = rng_global.choice(len(X_exp), max_train, replace=False)\n",
    "    X_exp_train = X_exp[idx]\n",
    "else:\n",
    "    X_exp_train = X_exp\n",
    "\n",
    "if len(X_mc_train) > max_train:\n",
    "    idx2 = rng_global.choice(len(X_mc_train), max_train, replace=False)\n",
    "    X_mc_train_small = X_mc_train[idx2]\n",
    "    w_mc_train_small = w_mc_train[idx2]\n",
    "else:\n",
    "    X_mc_train_small = X_mc_train\n",
    "    w_mc_train_small = w_mc_train\n",
    "\n",
    "print(\"Training reweighter on:\", X_exp_train.shape, \"exp vs\", X_mc_train_small.shape, \"mc (weights sum {:.1f})\".format(np.sum(w_mc_train_small)))\n",
    "\n",
    "model, clf_base = train_reweighter(X_mc_train_small, X_exp_train, w_mc_train_small)\n",
    "\n",
    "# apply to test set\n",
    "X_mc_test = mc_test_df[vars_features].values[mask_test] if 'mask_test' in globals() else mc_test_df[vars_features].values\n",
    "w_mc_test_orig = mc_test_df[\"weight\"].values[mask_test] if 'mask_test' in globals() else mc_test_df[\"weight\"].values\n",
    "w_test_new, w_test_raw = get_reweight_from_model(model, X_mc_test, w_mc_test_orig, clip=(clip_min, clip_max))\n",
    "\n",
    "# compute histograms and chi2\n",
    "data_hist, _ = np.histogram(exp_df[closure_var].values, bins=rue_edges)\n",
    "mc_hist_rew, w2 = weighted_hist_and_uncertainty(mc_test_df[closure_var].values[mask_test], w_test_new, rue_edges)\n",
    "sigma_data = np.sqrt(data_hist)\n",
    "sigma_mc_rew = np.sqrt(w2)\n",
    "chi2_val = chi2_between_histograms(data_hist, mc_hist_rew, sigma_data=sigma_data, sigma_mc=sigma_mc_rew)\n",
    "ndf = len(data_hist) - 1\n",
    "pval = chi2.sf(chi2_val, ndf)\n",
    "print(\"Nominal classifier reweight closure: chi2/ndf = {:.2f}/{}, p-value {:.3g}\".format(chi2_val, ndf, pval))\n",
    "\n",
    "# efficiency in interval\n",
    "mask_data = (exp_df[closure_var].values > cut_low) & (exp_df[closure_var].values < cut_high)\n",
    "eps_data = np.sum(mask_data) / np.sum(np.isfinite(exp_df[closure_var].values))\n",
    "mask_test_cut = (mc_test_df[closure_var].values[mask_test] > cut_low) & (mc_test_df[closure_var].values[mask_test] < cut_high)\n",
    "eps_mc_test_rew = np.sum(w_test_new[mask_test_cut]) / np.sum(w_test_new)\n",
    "print(\"eff_data = {:.5f}, eff_mc_test_rew = {:.5f}, ratio MC/Data = {:.3f}\".format(eps_data, eps_mc_test_rew, eps_mc_test_rew/eps_data))\n",
    "\n",
    "# plot comparison\n",
    "plt.figure(figsize=(8,4))\n",
    "bin_centers = 0.5*(rue_edges[:-1] + rue_edges[1:])\n",
    "plt.step(bin_centers, data_hist, where='mid', label='exp (counts)')\n",
    "plt.step(bin_centers, mc_hist_rew, where='mid', label='mc_rew (weighted counts)')\n",
    "plt.legend()\n",
    "plt.ylabel(\"counts\")\n",
    "plt.xlabel(closure_var)\n",
    "plt.title(\"Nominal classifier reweight closure\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- Bootstrap (index bootstrap for train sets) ----------\n",
    "chi2s = []\n",
    "effs = []\n",
    "raw_weights_records = []  # optional if you want to inspect distribution of raw reweights (memory heavy)\n",
    "\n",
    "for i in tqdm(range(Nboot), desc=\"bootstrap reweights\"):\n",
    "    # resample indices with replacement for training sets\n",
    "    idx_exp_b = rng_global.integers(0, len(X_exp), size=len(X_exp))\n",
    "    idx_mc_b = rng_global.integers(0, len(X_mc_train), size=len(X_mc_train))\n",
    "    X_exp_b = X_exp[idx_exp_b]\n",
    "    X_mc_b = X_mc_train[idx_mc_b]\n",
    "    w_mc_b = w_mc_train[idx_mc_b]\n",
    "\n",
    "    # train reweighter on bootstraped samples (we use smaller model to speed if needed)\n",
    "    try:\n",
    "        model_b, _ = train_reweighter(X_mc_b, X_exp_b, w_mc_b)\n",
    "    except Exception as e:\n",
    "        # fallback: skip this bootstrap\n",
    "        continue\n",
    "\n",
    "    # apply to test\n",
    "    w_test_b, _ = get_reweight_from_model(model_b, X_mc_test, w_mc_test_orig, clip=(clip_min, clip_max))\n",
    "    # hist and chi2\n",
    "    mc_hist_b, w2_b = weighted_hist_and_uncertainty(mc_test_df[closure_var].values[mask_test], w_test_b, rue_edges)\n",
    "    sigma_mc_b = np.sqrt(w2_b)\n",
    "    chi2_b = chi2_between_histograms(data_hist, mc_hist_b, sigma_data=sigma_data, sigma_mc=sigma_mc_b)\n",
    "    chi2s.append(chi2_b)\n",
    "    # efficiency\n",
    "    eff_b = np.sum(w_test_b[mask_test_cut]) / np.sum(w_test_b)\n",
    "    effs.append(eff_b)\n",
    "\n",
    "chi2s = np.array(chi2s)\n",
    "effs = np.array(effs)\n",
    "print(\"Bootstrap chi2 median {:.2f}, 16-84 pct: {:.2f} - {:.2f}\".format(np.median(chi2s), np.percentile(chi2s,16), np.percentile(chi2s,84)))\n",
    "print(\"Bootstrap eff median {:.5f}, 16-84 pct: {:.5f} - {:.5f}\".format(np.median(effs), np.percentile(effs,16), np.percentile(effs,84)))\n",
    "\n",
    "# Plot closure residual distribution for efficiency\n",
    "deltas = eps_data - effs\n",
    "plt.figure()\n",
    "plt.hist(deltas, bins=40, density=True, histtype='step', color='red')\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.xlabel(\"closure residual (eps_data - eps_mc_rew)\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.title(\"bootstrap closure residuals\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- Compare to histogram-based nominal (if you have nominal histogram wmap) ----------\n",
    "# If you computed histogram-based MC weights earlier (nominal_wmap), show overlay for recE:\n",
    "# compute mc_hist_histnom (apply nominal_wmap via bin indices precomputed similarly)\n",
    "# (left as an exercise; use weighted_hist_and_uncertainty with weights from hist-based pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f93a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training events: 1000000  -> nbins per axis: 27\n",
      "var r_Theta: bin range 0.0715 .. 1.55\n",
      "var age: bin range 0.632 .. 2.5\n",
      "var dr: bin range -32 .. 235\n",
      "Selected exp events: 940004  selected mc train events: 921188\n",
      "wmap stats: median 0.0423, mean 0.15, max 0.423, cap 0.423\n",
      "Selected mc_test events: 921206\n",
      "Nominal closure: chi2/ndf = 40992.70/48, p-value 0\n",
      "pull mean 18.740, std 22.032\n"
     ]
    }
   ],
   "source": [
    "exp_df = exp_df_sample\n",
    "mc_df = mc_df_sample\n",
    "mc_test_df = mc_test_df_sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# ----------------- 可调参数（请在运行前检查/修改） -----------------\n",
    "# variables to use for 3D reweight\n",
    "vars_3d = [\"r_Theta\", \"age\", \"dr\"]\n",
    "\n",
    "# percentile ranges to ignore extreme tails (avoid outliers controlling bins)\n",
    "pct_low, pct_high = 2.5, 100\n",
    "\n",
    "# desired approx number of bins per axis (fallback automatic)\n",
    "nbins_per_axis = None  # if None, will be auto-chosen based on MC training sample size\n",
    "\n",
    "# pseudo-count (Laplace prior) to add to hist bins to avoid zeros (small positive)\n",
    "alpha = 1e-3  \n",
    "\n",
    "# gaussian smoothing sigma (in bin units) applied on log(weight_map)\n",
    "# If sigma=0 -> no smoothing. Typical 0.5~1.5\n",
    "smooth_sigma = 1.0\n",
    "\n",
    "# cap factor to avoid huge weights: w_cap = median_w * cap_factor\n",
    "cap_factor = 10.0\n",
    "\n",
    "# bootstrap reps\n",
    "Nboot = 400\n",
    "\n",
    "# cut to test closure on (example: R_ue < 0.5; change as needed)\n",
    "cut_var = \"recE\"\n",
    "cut_value_1 = 1.79\n",
    "cut_value_2 = 2.13\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Helper: compute bin edges based on percentiles and nbins\n",
    "def make_bin_edges(series, nbins, lowpct=pct_low, highpct=pct_high):\n",
    "    lo = np.percentile(series, lowpct)\n",
    "    hi = np.percentile(series, highpct)\n",
    "    if lo == hi:\n",
    "        lo -= 1e-6\n",
    "        hi += 1e-6\n",
    "    return np.linspace(lo, hi, nbins + 1)\n",
    "\n",
    "# 1) choose nbins per axis if not provided\n",
    "N_train = len(mc_df)\n",
    "if nbins_per_axis is None:\n",
    "    # rule-of-thumb: prefer nbins so that total bins ~ N_train / 50 (i.e. ~50 events per bin)\n",
    "    # so nbins^3 ~ N_train/50 -> nbins ~ (N_train/50)**(1/3)\n",
    "    approx = int(np.clip(round((N_train / 50.0) ** (1/3)), 20, 50))\n",
    "    nbins_per_axis = max(4, approx)\n",
    "print(\"Training events:\", N_train, \" -> nbins per axis:\", nbins_per_axis)\n",
    "\n",
    "# 2) build bin edges from exp_df (we want to map MC onto experimental support)\n",
    "bin_edges = {}\n",
    "for v in vars_3d:\n",
    "    series = exp_df[v].dropna().values\n",
    "    bin_edges[v] = make_bin_edges(series, nbins_per_axis)\n",
    "    print(f\"var {v}: bin range {bin_edges[v][0]:.3g} .. {bin_edges[v][-1]:.3g}\")\n",
    "\n",
    "# 3) function to compute 3D hist (weighted)\n",
    "def hist3d(vals_dict, weights, edges):\n",
    "    # vals_dict: dict var->1D array length N\n",
    "    # edges: dict var->bin_edges\n",
    "    h, _ = np.histogramdd(np.vstack([vals_dict[v] for v in vars_3d]).T,\n",
    "                         bins=[edges[v] for v in vars_3d],\n",
    "                         weights=weights, density=False)\n",
    "    return h\n",
    "\n",
    "# 4) train histograms (use exp_df as numerator, mc_df as denominator with mc weight included)\n",
    "# Use only rows where all vars finite and within percentile range mapping\n",
    "def select_and_extract(df):\n",
    "    mask = np.ones(len(df), dtype=bool)\n",
    "    arrs = {}\n",
    "    for v in vars_3d:\n",
    "        arr = df[v].values\n",
    "        # clip to range (so binning consistent)\n",
    "        lo, hi = bin_edges[v][0], bin_edges[v][-1]\n",
    "        mask = mask & np.isfinite(arr) & (arr >= lo) & (arr <= hi)\n",
    "        arrs[v] = arr\n",
    "    return mask, arrs\n",
    "\n",
    "mask_exp, arrs_exp_all = select_and_extract(exp_df)\n",
    "mask_mc, arrs_mc_all = select_and_extract(mc_df)\n",
    "\n",
    "print(\"Selected exp events:\", mask_exp.sum(), \" selected mc train events:\", mask_mc.sum())\n",
    "\n",
    "# numerator: experimental histogram (unweighted counts)\n",
    "h_exp = hist3d({v: arrs_exp_all[v][mask_exp] for v in vars_3d},\n",
    "               weights=np.ones(mask_exp.sum()), edges=bin_edges)\n",
    "\n",
    "# denominator: mc train histogram using original mc weight\n",
    "mc_weights_train = mc_df[\"weight\"].values\n",
    "h_mc = hist3d({v: arrs_mc_all[v][mask_mc] for v in vars_3d},\n",
    "              weights=mc_weights_train[mask_mc], edges=bin_edges)\n",
    "\n",
    "# 5) form raw weight_map = (h_exp + alpha) / (h_mc + alpha)\n",
    "# add small alpha to numerator and denominator\n",
    "h_exp_reg = h_exp + alpha\n",
    "h_mc_reg = h_mc + alpha\n",
    "raw_wmap = h_exp_reg / h_mc_reg\n",
    "\n",
    "# 6) smoothing: work in log-space to avoid negative/zero issues\n",
    "with np.errstate(divide='ignore'):\n",
    "    log_w = np.log(raw_wmap)\n",
    "# mask infinite entries (shouldn't happen with alpha) and apply gaussian filter\n",
    "if smooth_sigma > 0:\n",
    "    log_w_smooth = gaussian_filter(log_w, sigma=smooth_sigma, mode='nearest')\n",
    "else:\n",
    "    log_w_smooth = log_w.copy()\n",
    "wmap = np.exp(log_w_smooth)\n",
    "\n",
    "# 7) cap weights to avoid outliers\n",
    "median_w = np.median(wmap[np.isfinite(wmap)])\n",
    "w_cap = median_w * cap_factor\n",
    "wmap_capped = np.where(wmap > w_cap, w_cap, wmap)\n",
    "\n",
    "# Save nominal wmap and info\n",
    "nominal_wmap = wmap_capped.copy()\n",
    "print(\"wmap stats: median {:.3g}, mean {:.3g}, max {:.3g}, cap {:.3g}\".format(\n",
    "    np.median(nominal_wmap), np.mean(nominal_wmap), np.max(nominal_wmap), w_cap))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8) apply nominal wmap to independent MC test set and compute closure metrics\n",
    "# ---------------------------------------------------------------------\n",
    "# prepare test arrays\n",
    "mask_test, arrs_test_all = select_and_extract(mc_test_df)\n",
    "print(\"Selected mc_test events:\", mask_test.sum())\n",
    "\n",
    "# get bin indices for mc_test\n",
    "coords_test = np.vstack([arrs_test_all[v][mask_test] for v in vars_3d]).T\n",
    "bin_idx_test = np.stack([\n",
    "    np.digitize(arrs_test_all[v][mask_test], bin_edges[v]) - 1 for v in vars_3d\n",
    "], axis=1)\n",
    "# clip indices\n",
    "for j, v in enumerate(vars_3d):\n",
    "    bin_idx_test[:, j] = np.clip(bin_idx_test[:, j], 0, nominal_wmap.shape[j]-1)\n",
    "\n",
    "# map each test event to bin weight\n",
    "w_orig_test = mc_test_df[\"weight\"].values[mask_test]\n",
    "wmap_flat_index = bin_idx_test[:,0]* (nominal_wmap.shape[1]*nominal_wmap.shape[2]) + \\\n",
    "                  bin_idx_test[:,1]* nominal_wmap.shape[2] + bin_idx_test[:,2]\n",
    "# faster: index via tuple\n",
    "event_w_rew = nominal_wmap[bin_idx_test[:,0], bin_idx_test[:,1], bin_idx_test[:,2]]\n",
    "# final test event weight = original mc weight * reweight factor\n",
    "w_test_final = w_orig_test * event_w_rew\n",
    "\n",
    "# build reweighted test histogram of R_ue to compare with experimental R_ue (we'll use same binning)\n",
    "# choose R_ue bins (percentile-based on exp)\n",
    "rue_edges = np.linspace(np.percentile(exp_df[cut_var], 1), np.percentile(exp_df[cut_var], 99), 50)\n",
    "data_hist_rue, _ = np.histogram(exp_df[cut_var].values, bins=rue_edges, density=False)\n",
    "mc_test_hist_rew, _ = np.histogram(mc_test_df[cut_var].values[mask_test], bins=rue_edges, weights=w_test_final, density=False)\n",
    "mc_test_hist_orig, _ = np.histogram(mc_test_df[cut_var].values[mask_test], bins=rue_edges, weights=w_orig_test, density=False)\n",
    "\n",
    "# compute per-bin uncertainties (data sqrt(N), weighted mc sqrt(sum w^2))\n",
    "sigma_data = np.sqrt(data_hist_rue)\n",
    "# compute sum of squared weights per bin\n",
    "w2_sums = np.zeros_like(mc_test_hist_rew)\n",
    "bin_idx_rue = np.digitize(mc_test_df[cut_var].values[mask_test], rue_edges) - 1\n",
    "for b in range(len(mc_test_hist_rew)):\n",
    "    sel = bin_idx_rue == b\n",
    "    w2_sums[b] = np.sum(w_test_final[sel]**2)\n",
    "sigma_mc_rew = np.sqrt(w2_sums)\n",
    "sigma_tot = np.sqrt(sigma_data**2 + sigma_mc_rew**2 + 1e-12)\n",
    "pulls = (data_hist_rue - mc_test_hist_rew) / sigma_tot\n",
    "chi2_val = np.sum(((data_hist_rue - mc_test_hist_rew)**2) / (sigma_tot**2))\n",
    "ndf = len(data_hist_rue) - 1\n",
    "pval = chi2.sf(chi2_val, ndf)\n",
    "print(\"Nominal closure: chi2/ndf = {:.2f}/{}, p-value {:.3g}\".format(chi2_val, ndf, pval))\n",
    "print(\"pull mean {:.3f}, std {:.3f}\".format(np.nanmean(pulls), np.nanstd(pulls)))\n",
    "\n",
    "# # efficiency for cut example: compute passing fraction for mc_test_rew vs exp (in control region)\n",
    "# exp_mask_for_eff = np.isfinite(exp_df[cut_var].values)\n",
    "# mask1=(exp_df[cut_var].values[exp_mask_for_eff] < cut_value_2)&(exp_df[cut_var].values[exp_mask_for_eff] > cut_value_1)\n",
    "# eps_data = np.sum(mask1) / np.sum(exp_mask_for_eff)\n",
    "# mask2=w_test_final[(mc_test_df[cut_var].values[mask_test] < cut_value_2)&(mc_test_df[cut_var].values[mask_test] > cut_value_1)]\n",
    "# eps_mc_test = np.sum(mask2) / np.sum(w_test_final)\n",
    "# print(\"eff_data = {:.4%}, eff_mc_test_rew = {:.4%}\".format(eps_data, eps_mc_test))\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # 9) Bootstrap: resample training sets and recompute wmap each time, apply to test, collect metrics\n",
    "# # ---------------------------------------------------------------------\n",
    "# def compute_wmap_from_train(train_mc_df, train_exp_df):\n",
    "#     # build hist from provided training samples (must be preselected within edges)\n",
    "#     mask_t_mc, arrs_t_mc = select_and_extract(train_mc_df)\n",
    "#     mask_t_exp, arrs_t_exp = select_and_extract(train_exp_df)\n",
    "#     h_exp_t = hist3d({v: arrs_t_exp[v][mask_t_exp] for v in vars_3d},\n",
    "#                      weights=np.ones(mask_t_exp.sum()), edges=bin_edges)\n",
    "#     h_mc_t = hist3d({v: arrs_t_mc[v][mask_t_mc] for v in vars_3d},\n",
    "#                     weights=train_mc_df[\"weight\"].values[mask_t_mc], edges=bin_edges)\n",
    "#     h_exp_t += alpha\n",
    "#     h_mc_t += alpha\n",
    "#     raw = h_exp_t / h_mc_t\n",
    "#     lograw = np.log(raw)\n",
    "#     smooth = gaussian_filter(lograw, sigma=smooth_sigma, mode='nearest')\n",
    "#     w = np.exp(smooth)\n",
    "#     # cap\n",
    "#     med = np.median(w[np.isfinite(w)])\n",
    "#     cap = med * cap_factor\n",
    "#     w = np.where(w > cap, cap, w)\n",
    "#     return w\n",
    "\n",
    "# # prepare arrays to store boot results\n",
    "# chi2s = []\n",
    "# effs_mc = []\n",
    "# # optionally store per-bin distributions (heavy mem) - we will store percentiles instead\n",
    "# # for performance, precompute test indices arrays\n",
    "# test_vals_arr = mc_test_df[cut_var].values[mask_test]\n",
    "# test_mask_len = mask_test.sum()\n",
    "# # Precompute: test bin index (for closure variable)\n",
    "# data_vals = exp_df[cut_var].values\n",
    "# test_vals = mc_test_df[cut_var].values[mask_test]\n",
    "# bin_idx_rue = np.digitize(test_vals, rue_edges) - 1\n",
    "# Nbins = len(rue_edges) - 1\n",
    "\n",
    "# def fast_sigma_mc(w):\n",
    "#     # bincount is much faster than loop\n",
    "#     return np.sqrt(np.bincount(bin_idx_rue, weights=w*w, minlength=Nbins))\n",
    "\n",
    "\n",
    "# chi2s = []\n",
    "# effs_mc = []\n",
    "# rng = np.random.default_rng(12345)\n",
    "# for i in tqdm(range(Nboot)):\n",
    "#     # Resample\n",
    "#     mc_boot = mc_df.sample(frac=1, replace=True, random_state=rng.integers(1e9))\n",
    "#     exp_boot = exp_df.sample(frac=1, replace=True, random_state=rng.integers(1e9))\n",
    "#     # Compute wmap\n",
    "#     wmap_b = compute_wmap_from_train(mc_boot, exp_boot)\n",
    "#     # Apply\n",
    "#     w_test_b = w_orig_test * wmap_b[bin_idx_test[:,0], bin_idx_test[:,1], bin_idx_test[:,2]]\n",
    "#     # Compute histogram (counts)\n",
    "#     mc_hist_b, _ = np.histogram(test_vals, bins=rue_edges, weights=w_test_b)\n",
    "#     # Uncertainty\n",
    "#     sigma_mc_b = fast_sigma_mc(w_test_b)\n",
    "#     sigma_tot_b = np.sqrt(sigma_data**2 + sigma_mc_b**2 + 1e-12)\n",
    "#     # χ²\n",
    "#     chi2_b = np.sum((data_hist_rue - mc_hist_b)**2 / (sigma_tot_b**2))\n",
    "#     chi2s.append(chi2_b)\n",
    "#     # Efficiency\n",
    "#     eff_mask = (test_vals < cut_value_2) & (test_vals > cut_value_1)\n",
    "#     eff_b = np.sum(w_test_b[eff_mask]) / np.sum(w_test_b)\n",
    "#     effs_mc.append(eff_b)\n",
    "\n",
    "# chi2s = np.array(chi2s)\n",
    "# effs_mc = np.array(effs_mc)\n",
    "\n",
    "# print(\"Bootstrap chi2 median {:.2f}, 16-84: {:.2f}-{:.2f}\".format(np.median(chi2s), np.percentile(chi2s,16), np.percentile(chi2s,84)))\n",
    "# print(\"Bootstrap eff_mc median {:.4f}, 16-84: {:.4f}-{:.4f}\".format(np.median(effs_mc), np.percentile(effs_mc,16), np.percentile(effs_mc,84)))\n",
    "\n",
    "# # Compute closure residual distribution: delta = eps_data - eps_mc_boot\n",
    "# deltas = eps_data - effs_mc\n",
    "# print(\"closure delta median {:.4f}, 16-84: {:.4f}-{:.4f}\".format(np.median(deltas), np.percentile(deltas,16), np.percentile(deltas,84)))\n",
    "\n",
    "\n",
    "# plt.hist(deltas, bins=20, density=True, histtype=\"step\", color=\"red\")\n",
    "# plt.axvline(0, color=\"k\", linestyle=\"--\")\n",
    "# plt.xlabel(\"closure residual\")\n",
    "# plt.ylabel(\"density\")\n",
    "# plt.savefig(\"./figures/closure_residual.png\")\n",
    "# plt.show()\n",
    "# interpret: if distribution centered near 0 and spread small -> good closure\n",
    "# otherwise take spread (e.g., 68% half-range) and/or median bias as modeling uncertainty\n",
    "\n",
    "# Save nominal wmap and percentiles (e.g., per bin 16/50/84) for shape nuisance creation if needed:\n",
    "# to compute percentiles per bin we'd need to store each boot wmap; that is memory heavy but doable if nbins small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6971413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp dr > 50: 0.7543098076909313\n",
      "MC dr > 50: 0.6934233121082676\n",
      "diff: -0.06088649558266368\n",
      "Exp age: 0.11434744077358555\n",
      "MC age: 0.1932981042105104\n",
      "diff: 0.07895066343692486\n",
      "Exp r_Theta: 0.9804395836029609\n",
      "MC r_Theta: 0.9313842408349879\n",
      "diff: -0.049055342767972965\n"
     ]
    }
   ],
   "source": [
    "dr_ratio_exp = np.sum(exp_df[\"dr\"] > 50) / exp_df.shape[0]\n",
    "dr_ratio_mc = np.sum(mc_df[\"dr\"] > 50) / mc_df.shape[0]\n",
    "print(f\"Exp dr > 50: {dr_ratio_exp}\")\n",
    "print(f\"MC dr > 50: {dr_ratio_mc}\")\n",
    "print(f\"diff: {dr_ratio_mc - dr_ratio_exp}\")\n",
    "\n",
    "age_radio_exp = np.sum((exp_df[\"age\"]>0.8)&(exp_df[\"age\"]<1.2)) / exp_df.shape[0]\n",
    "age_radio_mc = np.sum((mc_df[\"age\"]>0.8)&(mc_df[\"age\"]<1.2)) / mc_df.shape[0]\n",
    "print(f\"Exp age: {age_radio_exp}\")\n",
    "print(f\"MC age: {age_radio_mc}\")\n",
    "print(f\"diff: {age_radio_mc - age_radio_exp}\")\n",
    "\n",
    "r_Theta_radio_exp = np.sum((exp_df[\"r_Theta\"]<np.pi*50/180)) / exp_df.shape[0]\n",
    "r_Theta_radio_mc = np.sum((mc_df[\"r_Theta\"]<np.pi*50/180)) / mc_df.shape[0]\n",
    "print(f\"Exp r_Theta: {r_Theta_radio_exp}\")  \n",
    "print(f\"MC r_Theta: {r_Theta_radio_mc}\")\n",
    "print(f\"diff: {r_Theta_radio_mc - r_Theta_radio_exp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
